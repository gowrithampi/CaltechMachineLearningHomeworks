{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6 Using a neural network to fit the data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq4gQraBbK0HHZTFj1QG1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowrithampi/CaltechMachineLearningHomeworks/blob/master/Chapter_6_Using_a_neural_network_to_fit_the_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxZcdjh2ZYa"
      },
      "source": [
        "#Using a Neural Network to Fit the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRX1TO2a-Eaa"
      },
      "source": [
        "I am not trying to explain what a neural network is here, the textbook itself is great and there are lot of other sources, but I will attempt to distill some of the insights from this book that I felt were far better done in this book than in some other sources I've read. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlXxuJ-p2lfz"
      },
      "source": [
        "The following is my attempt to provide some intutition around one of the reasons deep learning is useful, namely the lack of a need feature engineering. The intuition was gained by reading this book and several other works. \n",
        "\n",
        "When we build a model for a process, we assume we know something about the process and we can approximate it with a set of equations or a probability distribution, which has some parameters. \n",
        "Once we lay out these equations in terms of the parameters, we can use the data observed from an experiment (or real world data) to estimate these parameters. \n",
        "\n",
        "For example in the previous case, where we wanted to calibrate the thermometer, we assumed a linear relationship between the celsius measurements and the uncalibrated thermometer reading. This model worked well for us but we were assuming we knew the following: \n",
        "\n",
        "1. The nature of the relationship between the celsius scale and the unknown scale (linear) \n",
        "2. All the variables needed to calibrate the unknown scale (what if the unknown scale depended on humidity as well and wasn't purely a temperature reading?) \n",
        "\n",
        "Selecting the variables in a model, for eg: nutrition, age for weight of a baby, is a large part of what is known as feature engineering. Good feature engineering is needed for the success of a model. Incompletely specified models(badly engineered, with some variables left out, may lead to issues like endogeneity, (think leaving out mother's health as a variable in predicting the weight of a baby) \n",
        "Feature engineering requires expert domain knowledge, exploratory data analysis. the exact functional form of the model (polynomial terms) etc also need deep knowledge of the system which may not exist for complex systems. \n",
        "\n",
        "Deep learning takes care of the feature engineering problem, because artificial neural networks are very good at approximating extremely complex functions, just from data. \n",
        "\n",
        "Let us now look at neurons and neural networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE_k40xegezM"
      },
      "source": [
        "## Artificial Neurons\n",
        "loosely inspired by the working of the human brain, aritifical neurons are at its core a linear transofrmation followed by a non linear *activation function*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYMWA41_ho7a"
      },
      "source": [
        "The Neuron: $o = tanh(w*x+b)$\n",
        "\n",
        "$x$ *is the input*\n",
        "\n",
        "$w$ *and* $b$ *are the learned parameters*\n",
        "\n",
        "$w*x+b$ is the linear transformation \n",
        "\n",
        "$tanh$ is the non linear activation function \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbZP4TBi11h"
      },
      "source": [
        "The non linear activation function serves two purposes, which are important for approximating complex functions: \n",
        "\n",
        "1. The non linearity means the slope of the output with respect to the parameter( a weight) can change depending on the value of the output. \n",
        "\n",
        "2. The bounds provided by many of the activation functions, ensures that the output remains within a range. For eg: the output of a tanh funciton remains between -1 and +1\n",
        "\n",
        "The output is sensitive to the weights only in a particular region. Once the activation function saturates a change in the weights has little to no impact on the output. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z3JNu5h_7xN"
      },
      "source": [
        "## Neural Networks using Pytorch\n",
        "\n",
        "In pytorch the base module/class for all neural networks is torch.nn\n",
        "For building any neural network we subclass from torch.nn \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69mzvrOm1gL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30099cdd-dc41-4f1b-dafc-22f08181b9a2"
      },
      "source": [
        "## Pytorch's NN module is dedicated to neural networks. \n",
        "## To propagate a neural network forward we use the module's call function rather \n",
        "## than the forward function. \n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "t_un_val = torch.tensor([1], dtype = torch.float32)\n",
        "\n",
        "## the 1,1 arguments to the constructor represents a layer with 1 input and 1 output\n",
        "linear_model = nn.Linear(1,1)\n",
        "linear_model(t_un_val)\n",
        "\n",
        "## the tensor input here has a dimensionality of 1, ideally the dimensionality should be 2 where one of the dimensions is the batch size.\n",
        "## Inputs are very often batched for neural networks. "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5706], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKfxKoKDs_g"
      },
      "source": [
        "## Batching inputs \n",
        "## Batching is done for multiple reasons: \n",
        "## 1 of them is to saturate GPU resources, GPUs parallelize. Parallelization is most effective when we give them multiple ( a batch of inputs)\n",
        "## My hunch is that the other reason is, the variation of the error function is lower when inputs are batched. \n",
        "\n",
        "t_u = [0.5,14.0,28.0,11.0,8.0,3.0,-4.0,6.0,13.0,21.0]\n",
        "t_c = [35.7,55.9,58.2,81.9,56.3,48.9,21.9,48.4,60.4,68.4]\n",
        "\n",
        "# we add an unsqueeze function here, to add an extra dimension, the dimension should be [10 * 1] , 10 batches and 1 input\n",
        "\n",
        "t_c = torch.tensor(t_c).unsqueeze(1)\n",
        "t_u = torch.tensor(t_u).unsqueeze(1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3y8nE1MBx1"
      },
      "source": [
        "### Note: Overloading of the term *dimension* \n",
        "\n",
        "I think the word dimension is terribly overloaded in Deep learning. \n",
        "For eg: as a linear algebra student, a 5 dimensional vector is something like [1,2.1, 3.1, 4.1] That is this vector is a point in 5 dimensional space. So a vector which represented heights and weights would be two dimensional [1.7 , 70] (1.7m tall and 70kg weight) \n",
        "\n",
        "In Deep learning dimension usually refers not to the number of features (like height and weight) but to the shape of the input. \n",
        "for eg: taking just height and weight of 3 people (batch size of 3) \n",
        "\n",
        "[[1.7 , 70]\n",
        "\n",
        "[1.8 , 90]\n",
        "\n",
        "[1.5 , 60]]\n",
        "\n",
        "The shape of this matrix is 2 and this is referred to as the input dimension in deep learning. \n",
        "\n",
        "If anyone has a counter view please email me! \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ_TwtyMNegK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}